{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a79a4c3c3d61>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.chdir(\"C:\\\\Users\\\\Taha\\\\Desktop\\\\Hate Speach\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data=pd.read_csv(\"train.csv\")\n",
    "data=main_data.copy()\n",
    "data.drop(columns=['id'],axis=1,inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label\n",
       "0  29720\n",
       "1   2242"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s : 1s  ::   13.26 : 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Check class distribution in dependent variable \n",
    "display(data['label'].value_counts().to_frame())\n",
    "print(\"0s : 1s  ::  \",(data['label'].value_counts()[0]/data['label'].value_counts()[1]).round(2),\": 1\")\n",
    "plt.bar([0,1],data['label'].value_counts())\n",
    "plt.title(\"class proportions in the dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we see that we have an unbalanced data mojority class to minority class ratio of 13 : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Balancing the dataset using Oversampling\n",
    "data1=data[data['label']==1]\n",
    "data0=data[data['label']==0]\n",
    "data=pd.concat([data,data1,data1], axis=0)\n",
    "data\n",
    "\n",
    "#Check class distribution in dependent variable again\n",
    "display(data['label'].value_counts().to_frame())\n",
    "print(\"0s : 1s  ::  \",(data['label'].value_counts()[0]/data['label'].value_counts()[1]).round(2),\": 1\")\n",
    "plt.bar([0,1],data['label'].value_counts())\n",
    "plt.title(\"class proportions in the dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "def clean_text(text ): \n",
    "    delete_dict = {sp_character: '' for sp_character in string.punctuation} \n",
    "    delete_dict[' '] = ' ' \n",
    "    table = str.maketrans(delete_dict)\n",
    "    text1 = text.translate(table)\n",
    "    textArr= text1.split()\n",
    "    text2 = ' '.join([w for w in textArr if ( not w.isdigit() and  ( not w.isdigit() and len(w)>3))]) \n",
    "    \n",
    "    return text2.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess train dataset\n",
    "data['tweet'] = data['tweet'].apply(remove_emoji)\n",
    "data['tweet'] = data['tweet'].apply(clean_text)\n",
    "data['Num_words_text'] = data['tweet'].apply(lambda x:len(str(x).split())) \n",
    "\n",
    "train_data,test_data= train_test_split(data, test_size=0.2)\n",
    "train_data.reset_index(drop=True,inplace=True)\n",
    "test_data.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========Train Data =========\n",
      "0    23761\n",
      "1     5395\n",
      "Name: label, dtype: int64\n",
      "29156\n",
      "==============================\n",
      "===========Test Data =========\n",
      "0    5959\n",
      "1    1331\n",
      "Name: label, dtype: int64\n",
      "7290\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "#classes proportion in dependent variable in train and test dataset\n",
    "print('===========Train Data =========')\n",
    "print(train_data['label'].value_counts())\n",
    "print(len(train_data))\n",
    "print('==============================')\n",
    "\n",
    "print('===========Test Data =========')\n",
    "print(test_data['label'].value_counts())\n",
    "print(len(test_data))\n",
    "print('==============================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data len:23324\n",
      "Class distributionCounter({0: 19008, 1: 4316})\n",
      "Valid data len:5832\n",
      "Class distributionCounter({0: 4753, 1: 1079})\n"
     ]
    }
   ],
   "source": [
    "#train and validation dataset splitting\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_data['tweet'].tolist(),\\\n",
    "                                                      train_data['label'].tolist(),\\\n",
    "                                                      test_size=0.2,\\\n",
    "                                                      stratify = train_data['label'].tolist(),\\\n",
    "                                                      random_state=0)\n",
    "\n",
    "\n",
    "print('Train data len:'+str(len(X_train)))\n",
    "print('Class distribution'+str(Counter(y_train)))\n",
    "print('Valid data len:'+str(len(X_valid)))\n",
    "print('Class distribution'+ str(Counter(y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 50000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words,oov_token=\"unk\")\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-8e656ef6a59b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#convert sentences to sequences of numbers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mx_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mx_test\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "#convert sentences to sequences of numbers\n",
    "x_train = np.array( tokenizer.texts_to_sequences(X_train) )\n",
    "x_valid = np.array( tokenizer.texts_to_sequences(X_valid) )\n",
    "x_test  = np.array( tokenizer.texts_to_sequences(test_data['tweet'].tolist()) )\n",
    "\n",
    "#padding \n",
    "maxlen=50\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
    "x_valid = pad_sequences(x_valid, padding='post', maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(y_train)\n",
    "valid_labels = np.asarray(y_valid)\n",
    "test_labels = np.asarray(test_data['label'].tolist())\n",
    "\n",
    "\n",
    "print('Train data len:'+str(len(x_train)))\n",
    "print('Class distribution'+str(Counter(train_labels)))\n",
    "\n",
    "print('Validation data len:'+str(len(x_valid)))\n",
    "print('Class distribution'+str(Counter(valid_labels)))\n",
    "\n",
    "print('Test data len:'+str(len(x_test)))\n",
    "print('Class distribution'+str(Counter(test_labels)))\n",
    "\n",
    "#tensorflow dataset preparation\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train,train_labels))\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices((x_valid,valid_labels))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test,test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Train dataset ====\n",
      "tf.Tensor(\n",
      "[   2    2 2163    8  327  508   11 2164  193 1495   13 3115  166 2322\n",
      "  599    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0], shape=(50,), dtype=int32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "======Validation dataset ====\n",
      "tf.Tensor(\n",
      "[   2    1  228 2685   11   38   11 3056  942    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0], shape=(50,), dtype=int32) tf.Tensor(0, shape=(), dtype=int32)\n",
      "======Test dataset ====\n",
      "tf.Tensor(\n",
      "[209   5   1 584   1   1   1   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0], shape=(50,), dtype=int32) tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "count =0\n",
    "print('======Train dataset ====')\n",
    "for value,label in train_ds:\n",
    "    count += 1\n",
    "    print(value,label)\n",
    "    if count==1:\n",
    "        break\n",
    "count =0\n",
    "print('======Validation dataset ====')\n",
    "for value,label in valid_ds:\n",
    "    count += 1\n",
    "    print(value,label)\n",
    "    if count==1:\n",
    "        break\n",
    "count =0\n",
    "print('======Test dataset ====')\n",
    "for value,label in test_ds:\n",
    "    count += 1\n",
    "    print(value,label)\n",
    "    if count==1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'maxlen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-7a9d30426fdf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m50000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0membedding_dim\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msequence_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'maxlen' is not defined"
     ]
    }
   ],
   "source": [
    "#model preparation\n",
    "max_features =50000\n",
    "embedding_dim =16\n",
    "sequence_length = maxlen\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(max_features +1, embedding_dim, input_length=sequence_length,\\\n",
    "                                    embeddings_regularizer = regularizers.l2(0.005))) \n",
    "model.add(tf.keras.layers.Dropout(0.4))\n",
    "\n",
    "model.add(tf.keras.layers.LSTM(embedding_dim,dropout=0.2, recurrent_dropout=0.2,return_sequences=True,\\\n",
    "                                                             kernel_regularizer=regularizers.l2(0.005),\\\n",
    "                                                             bias_regularizer=regularizers.l2(0.005)))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(512, activation='relu',\\\n",
    "                                kernel_regularizer=regularizers.l2(0.001),\\\n",
    "                                bias_regularizer=regularizers.l2(0.001),))\n",
    "model.add(tf.keras.layers.Dropout(0.4))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu',\\\n",
    "                                kernel_regularizer=regularizers.l2(0.001),\\\n",
    "                                bias_regularizer=regularizers.l2(0.001),))\n",
    "model.add(tf.keras.layers.Dropout(0.4))\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n",
    "                               \n",
    "\n",
    "\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),optimizer=tf.keras.optimizers.Adam(1e-3),metrics=[tf.keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "23/23 [==============================] - 7s 300ms/step - loss: 2.9764 - binary_accuracy: 0.7903 - val_loss: 1.5497 - val_binary_accuracy: 0.8150\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 7s 298ms/step - loss: 1.1182 - binary_accuracy: 0.8149 - val_loss: 0.7838 - val_binary_accuracy: 0.8150\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 5s 233ms/step - loss: 0.7348 - binary_accuracy: 0.8149 - val_loss: 0.6434 - val_binary_accuracy: 0.8150\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 5s 221ms/step - loss: 0.6541 - binary_accuracy: 0.8149 - val_loss: 0.5848 - val_binary_accuracy: 0.8150\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 5s 228ms/step - loss: 0.5887 - binary_accuracy: 0.8217 - val_loss: 0.4819 - val_binary_accuracy: 0.8671\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 6s 248ms/step - loss: 0.4866 - binary_accuracy: 0.8903 - val_loss: 0.3421 - val_binary_accuracy: 0.9360\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 6s 249ms/step - loss: 0.4013 - binary_accuracy: 0.9392 - val_loss: 0.2920 - val_binary_accuracy: 0.9528\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 6s 249ms/step - loss: 0.3507 - binary_accuracy: 0.9599 - val_loss: 0.2639 - val_binary_accuracy: 0.9609\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 6s 251ms/step - loss: 0.3200 - binary_accuracy: 0.9676 - val_loss: 0.2676 - val_binary_accuracy: 0.9542\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 6s 247ms/step - loss: 0.3029 - binary_accuracy: 0.9735 - val_loss: 0.2425 - val_binary_accuracy: 0.9618\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "# Fit the model using the train and test datasets.\n",
    "history = model.fit(train_ds.shuffle(5000).batch(1024),\n",
    "                    epochs= epochs ,\n",
    "                    validation_data=valid_ds.batch(1024),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.4268547e-01]\n",
      " [9.7826517e-01]\n",
      " [5.8067143e-03]\n",
      " [2.8772950e-03]\n",
      " [3.6336260e-05]\n",
      " [1.2026131e-03]\n",
      " [9.8628443e-01]\n",
      " [4.0706992e-04]\n",
      " [1.5170112e-01]\n",
      " [3.8477778e-04]]\n"
     ]
    }
   ],
   "source": [
    "#make predictions on validation dataset\n",
    "valid_predict= model.predict(x_valid)\n",
    "print(valid_predict[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-78bc6703b6ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#model's metrics on test dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx_test\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'post'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "#model's metrics on test dataset\n",
    "x_test  = np.array( tokenizer.texts_to_sequences(test_data['tweet'].tolist()) )\n",
    "x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "\n",
    "#Generate predictions for all samples\n",
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-4709c647fee7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#plot predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0max1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0max1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0max2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "#plot predictions\n",
    "f, (ax1, ax2) = plt.subplots(1, 2,figsize=(15,5))\n",
    "ax1.scatter(predictions,range(0,len(predictions)),alpha=0.2)\n",
    "ax2=sns.distplot(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mpl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-a385710ae689>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_aspect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'equal'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mmpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'figure.figsize'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mcolors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'axes.prop_cycle'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mby_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'color'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mpl' is not defined"
     ]
    }
   ],
   "source": [
    "#decide the cutoff for classifying the predicted probabilities as 1 or 0\n",
    "def plot_roc(name, labels, predictions, **kwargs):\n",
    "    fp, tp, thresholds = sklearn.metrics.roc_curve(labels, predictions)\n",
    "    plt.plot(fp, tp, label=name, linewidth=2, **kwargs)\n",
    "    plt.xlabel('False positives Rate')\n",
    "    plt.ylabel('True positives Rate')\n",
    "    plt.xlim([-0.03, 1.0])\n",
    "    plt.ylim([0.0, 1.03])\n",
    "    plt.grid(True)\n",
    "    thresholdsLength = len(thresholds)\n",
    "    thresholds_every = 1000\n",
    "    colorMap = plt.get_cmap('jet', thresholdsLength)\n",
    "    for i in range(0, thresholdsLength, thresholds_every):\n",
    "        threshold_value_with_max_four_decimals = str(thresholds[i])[:5]\n",
    "        plt.text(fp[i] - 0.03, tp[i] + 0.001, threshold_value_with_max_four_decimals, fontdict={'size': 15}, color=colorMap(i/thresholdsLength));\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (7,7)\n",
    "\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "plot_roc(\"Valid Baseline\", valid_labels, valid_predict, color=colors[0], linestyle='--')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-daf7302e1571>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.86\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pred_sentiment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pred_sentiment'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpred_sentiment\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpred_sentiment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pred_sentiment'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpred_sentiment\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpred_sentiment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "cutoff=0.86\n",
    "test_data['pred_sentiment']= predictions\n",
    "test_data['pred_sentiment'] = np.where((test_data.pred_sentiment >= cutoff),1,test_data.pred_sentiment)\n",
    "test_data['pred_sentiment'] = np.where((test_data.pred_sentiment < cutoff),0,test_data.pred_sentiment)\n",
    "\n",
    "labels = [0, 1]\n",
    "print(classification_report(test_data['label'].tolist(),test_data['pred_sentiment'].tolist(),labels=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING MODEL ON THE TEST DATASET PROVIDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'remove_emoji' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-b61f6dd30b38>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mftest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mftest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mftest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremove_emoji\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mftest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mftest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'remove_emoji' is not defined"
     ]
    }
   ],
   "source": [
    "final_test=pd.read_csv(\"test.csv\")\n",
    "\n",
    "ftest=final_test.copy()\n",
    "ftest.drop(columns=['id'],axis=1,inplace=True)\n",
    "\n",
    "ftest['tweet'] = ftest['tweet'].apply(remove_emoji)\n",
    "ftest['tweet'] = ftest['tweet'].apply(clean_text)\n",
    "\n",
    "f_test  = np.array( tokenizer.texts_to_sequences(ftest['tweet'].tolist()) )\n",
    "f_test = pad_sequences(f_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "display((x_test))\n",
    "display((f_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-496f8131610a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#predict on actual test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#predict on actual test data\n",
    "predictions = model.predict(f_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-4a5f77806a89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#plot predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0max1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0max1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mftest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0max2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "#plot predictions\n",
    "f, (ax1, ax2) = plt.subplots(1, 2,figsize=(15,5))\n",
    "ax1.scatter(predictions,ftest.index,alpha=0.2)\n",
    "ax2=sns.distplot(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>pred_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user white supremacists want everyone birdsâ movie hereâs</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>thought factory neutrality right wing fascism politics media brexit trump leadership</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>chick gets fucked hottest naked lady</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>suppo taiji fisherman bullying racism tweet4taiji thecove seashepherd</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>user user trumps invested billions into saudi empowers people funding isis trumpsahypocrite</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17148</th>\n",
       "      <td>grew fucked upð¤ fucked upð¥ believing better place fucking upð© âï¸</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17176</th>\n",
       "      <td>user user most racist ever</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17188</th>\n",
       "      <td>black professor demonizes proposes nazi style confiscation white assets like 1930s germany breaking</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17192</th>\n",
       "      <td>thought factory leftright polarisation trump uselections2016 leadership politics brexit</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17197</th>\n",
       "      <td>fuck bitch pedophile hole sucker</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>997 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                     tweet  \\\n",
       "1                                            user white supremacists want everyone birdsâ movie hereâs   \n",
       "19                    thought factory neutrality right wing fascism politics media brexit trump leadership   \n",
       "26                                                                    chick gets fucked hottest naked lady   \n",
       "33                                   suppo taiji fisherman bullying racism tweet4taiji thecove seashepherd   \n",
       "42             user user trumps invested billions into saudi empowers people funding isis trumpsahypocrite   \n",
       "...                                                                                                    ...   \n",
       "17148                        grew fucked upð¤ fucked upð¥ believing better place fucking upð© âï¸   \n",
       "17176                                                                           user user most racist ever   \n",
       "17188  black professor demonizes proposes nazi style confiscation white assets like 1930s germany breaking   \n",
       "17192              thought factory leftright polarisation trump uselections2016 leadership politics brexit   \n",
       "17197                                                                     fuck bitch pedophile hole sucker   \n",
       "\n",
       "       pred_sentiment  \n",
       "1                 1.0  \n",
       "19                1.0  \n",
       "26                1.0  \n",
       "33                1.0  \n",
       "42                1.0  \n",
       "...               ...  \n",
       "17148             1.0  \n",
       "17176             1.0  \n",
       "17188             1.0  \n",
       "17192             1.0  \n",
       "17197             1.0  \n",
       "\n",
       "[997 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mapping prediction to 1 or 0\n",
    "ftest['pred_sentiment']= predictions\n",
    "ftest['pred_sentiment'] = np.where((ftest.pred_sentiment >= cutoff),1,ftest.pred_sentiment)\n",
    "ftest['pred_sentiment'] = np.where((ftest.pred_sentiment < cutoff),0,ftest.pred_sentiment)\n",
    "\n",
    "#processed tweets categorized as hate speech\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "ftest[ftest['pred_sentiment']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'pred_sentiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\rt210\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2888\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2889\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2890\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'pred_sentiment'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-3cef37dff7d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#actual tweets categorized as hate speech\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfinal_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mftest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mftest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pred_sentiment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\rt210\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2897\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2898\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2899\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2900\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rt210\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2889\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2890\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2891\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2893\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'pred_sentiment'"
     ]
    }
   ],
   "source": [
    "#actual tweets categorized as hate speech\n",
    "final_test.iloc[ftest[ftest['pred_sentiment']==1].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-4282c78ed391>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'finalized_model.pkl'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "filename = 'finalized_model.pkl'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
