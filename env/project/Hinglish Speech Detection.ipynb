{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data=pd.read_csv(\"train.csv\")\n",
    "data=main_data.copy()\n",
    "data.drop(columns=['id'],axis=1,inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check class distribution in dependent variable \n",
    "display(data['label'].value_counts().to_frame())\n",
    "print(\"0s : 1s  ::  \",(data['label'].value_counts()[0]/data['label'].value_counts()[1]).round(2),\": 1\")\n",
    "plt.bar([0,1],data['label'].value_counts())\n",
    "plt.title(\"class proportions in the dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Balancing the dataset using Oversampling\n",
    "data1=data[data['label']==1]\n",
    "data0=data[data['label']==0]\n",
    "data=pd.concat([data,data1,data1], axis=0)\n",
    "data\n",
    "\n",
    "#Check class distribution in dependent variable again\n",
    "display(data['label'].value_counts().to_frame())\n",
    "print(\"0s : 1s  ::  \",(data['label'].value_counts()[0]/data['label'].value_counts()[1]).round(2),\": 1\")\n",
    "plt.bar([0,1],data['label'].value_counts())\n",
    "plt.title(\"class proportions in the dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "def clean_text(text ): \n",
    "    delete_dict = {sp_character: '' for sp_character in string.punctuation} \n",
    "    delete_dict[' '] = ' ' \n",
    "    table = str.maketrans(delete_dict)\n",
    "    text1 = text.translate(table)\n",
    "    textArr= text1.split()\n",
    "    text2 = ' '.join([w for w in textArr if ( not w.isdigit() and  ( not w.isdigit() and len(w)>3))]) \n",
    "    \n",
    "    return text2.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess train dataset\n",
    "data['tweet'] = data['tweet'].apply(remove_emoji)\n",
    "data['tweet'] = data['tweet'].apply(clean_text)\n",
    "data['Num_words_text'] = data['tweet'].apply(lambda x:len(str(x).split())) \n",
    "\n",
    "train_data,test_data= train_test_split(data, test_size=0.2)\n",
    "train_data.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classes proportion in dependent variable in train and test dataset\n",
    "print('===========Train Data =========')\n",
    "print(train_data['label'].value_counts())\n",
    "print(len(train_data))\n",
    "print('==============================')\n",
    "\n",
    "print('===========Test Data =========')\n",
    "print(test_data['label'].value_counts())\n",
    "print(len(test_data))\n",
    "print('==============================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and validation dataset splitting\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_data['tweet'].tolist(),\\\n",
    "                                                      train_data['label'].tolist(),\\\n",
    "                                                      test_size=0.2,\\\n",
    "                                                      stratify = train_data['label'].tolist(),\\\n",
    "                                                      random_state=0)\n",
    "\n",
    "\n",
    "print('Train data len:'+str(len(X_train)))\n",
    "print('Class distribution'+str(Counter(y_train)))\n",
    "print('Valid data len:'+str(len(X_valid)))\n",
    "print('Class distribution'+ str(Counter(y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 50000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words,oov_token=\"unk\")\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert sentences to sequences of numbers\n",
    "x_train = np.array( tokenizer.texts_to_sequences(X_train) )\n",
    "x_valid = np.array( tokenizer.texts_to_sequences(X_valid) )\n",
    "x_test  = np.array( tokenizer.texts_to_sequences(test_data['tweet'].tolist()) )\n",
    "\n",
    "#padding \n",
    "maxlen=50\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
    "x_valid = pad_sequences(x_valid, padding='post', maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(y_train)\n",
    "valid_labels = np.asarray(y_valid)\n",
    "test_labels = np.asarray(test_data['label'].tolist())\n",
    "\n",
    "\n",
    "print('Train data len:'+str(len(x_train)))\n",
    "print('Class distribution'+str(Counter(train_labels)))\n",
    "\n",
    "print('Validation data len:'+str(len(x_valid)))\n",
    "print('Class distribution'+str(Counter(valid_labels)))\n",
    "\n",
    "print('Test data len:'+str(len(x_test)))\n",
    "print('Class distribution'+str(Counter(test_labels)))\n",
    "\n",
    "#tensorflow dataset preparation\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train,train_labels))\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices((x_valid,valid_labels))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test,test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count =0\n",
    "print('======Train dataset ====')\n",
    "for value,label in train_ds:\n",
    "    count += 1\n",
    "    print(value,label)\n",
    "    if count==1:\n",
    "        break\n",
    "count =0\n",
    "print('======Validation dataset ====')\n",
    "for value,label in valid_ds:\n",
    "    count += 1\n",
    "    print(value,label)\n",
    "    if count==1:\n",
    "        break\n",
    "count =0\n",
    "print('======Test dataset ====')\n",
    "for value,label in test_ds:\n",
    "    count += 1\n",
    "    print(value,label)\n",
    "    if count==1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model preparation\n",
    "max_features =50000\n",
    "embedding_dim =16\n",
    "sequence_length = maxlen\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(max_features +1, embedding_dim, input_length=sequence_length,\\\n",
    "                                    embeddings_regularizer = regularizers.l2(0.005))) \n",
    "model.add(tf.keras.layers.Dropout(0.4))\n",
    "\n",
    "model.add(tf.keras.layers.LSTM(embedding_dim,dropout=0.2, recurrent_dropout=0.2,return_sequences=True,\\\n",
    "                                                             kernel_regularizer=regularizers.l2(0.005),\\\n",
    "                                                             bias_regularizer=regularizers.l2(0.005)))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(512, activation='relu',\\\n",
    "                                kernel_regularizer=regularizers.l2(0.001),\\\n",
    "                                bias_regularizer=regularizers.l2(0.001),))\n",
    "model.add(tf.keras.layers.Dropout(0.4))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu',\\\n",
    "                                kernel_regularizer=regularizers.l2(0.001),\\\n",
    "                                bias_regularizer=regularizers.l2(0.001),))\n",
    "model.add(tf.keras.layers.Dropout(0.4))\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n",
    "                               \n",
    "\n",
    "\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),optimizer=tf.keras.optimizers.Adam(1e-3),metrics=[tf.keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "# Fit the model using the train and test datasets.\n",
    "history = model.fit(train_ds.shuffle(5000).batch(1024),\n",
    "                    epochs= epochs ,\n",
    "                    validation_data=valid_ds.batch(1024),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions on validation dataset\n",
    "valid_predict= model.predict(x_valid)\n",
    "print(valid_predict[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model's metrics on test dataset\n",
    "x_test  = np.array( tokenizer.texts_to_sequences(test_data['tweet'].tolist()) )\n",
    "x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "\n",
    "#Generate predictions for all samples\n",
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot predictions\n",
    "f, (ax1, ax2) = plt.subplots(1, 2,figsize=(15,5))\n",
    "ax1.scatter(predictions,range(0,len(predictions)),alpha=0.2)\n",
    "ax2=sns.distplot(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decide the cutoff for classifying the predicted probabilities as 1 or 0\n",
    "def plot_roc(name, labels, predictions, **kwargs):\n",
    "    fp, tp, thresholds = sklearn.metrics.roc_curve(labels, predictions)\n",
    "    plt.plot(fp, tp, label=name, linewidth=2, **kwargs)\n",
    "    plt.xlabel('False positives Rate')\n",
    "    plt.ylabel('True positives Rate')\n",
    "    plt.xlim([-0.03, 1.0])\n",
    "    plt.ylim([0.0, 1.03])\n",
    "    plt.grid(True)\n",
    "    thresholdsLength = len(thresholds)\n",
    "    thresholds_every = 1000\n",
    "    colorMap = plt.get_cmap('jet', thresholdsLength)\n",
    "    for i in range(0, thresholdsLength, thresholds_every):\n",
    "        threshold_value_with_max_four_decimals = str(thresholds[i])[:5]\n",
    "        plt.text(fp[i] - 0.03, tp[i] + 0.001, threshold_value_with_max_four_decimals, fontdict={'size': 15}, color=colorMap(i/thresholdsLength));\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (7,7)\n",
    "\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "plot_roc(\"Valid Baseline\", valid_labels, valid_predict, color=colors[0], linestyle='--')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff=0.86\n",
    "test_data['pred_sentiment']= predictions\n",
    "test_data['pred_sentiment'] = np.where((test_data.pred_sentiment >= cutoff),1,test_data.pred_sentiment)\n",
    "test_data['pred_sentiment'] = np.where((test_data.pred_sentiment < cutoff),0,test_data.pred_sentiment)\n",
    "\n",
    "labels = [0, 1]\n",
    "print(classification_report(test_data['label'].tolist(),test_data['pred_sentiment'].tolist(),labels=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test=pd.read_csv(\"test.csv\")\n",
    "\n",
    "ftest=final_test.copy()\n",
    "ftest.drop(columns=['id'],axis=1,inplace=True)\n",
    "\n",
    "ftest['tweet'] = ftest['tweet'].apply(remove_emoji)\n",
    "ftest['tweet'] = ftest['tweet'].apply(clean_text)\n",
    "\n",
    "f_test  = np.array( tokenizer.texts_to_sequences(ftest['tweet'].tolist()) )\n",
    "f_test = pad_sequences(f_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "display((x_test))\n",
    "display((f_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on actual test data\n",
    "predictions = model.predict(f_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot predictions\n",
    "f, (ax1, ax2) = plt.subplots(1, 2,figsize=(15,5))\n",
    "ax1.scatter(predictions,ftest.index,alpha=0.2)\n",
    "ax2=sns.distplot(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping prediction to 1 or 0\n",
    "ftest['pred_sentiment']= predictions\n",
    "ftest['pred_sentiment'] = np.where((ftest.pred_sentiment >= cutoff),1,ftest.pred_sentiment)\n",
    "ftest['pred_sentiment'] = np.where((ftest.pred_sentiment < cutoff),0,ftest.pred_sentiment)\n",
    "\n",
    "#processed tweets categorized as hate speech\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "ftest[ftest['pred_sentiment']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actual tweets categorized as hate speech\n",
    "final_test.iloc[ftest[ftest['pred_sentiment']==1].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pac' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5227ec6d14fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfileame\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'finalized_model.pkl'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpac\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pac' is not defined"
     ]
    }
   ],
   "source": [
    "#save model\n",
    "import pickle\n",
    "fileame='finalized_model.pkl'\n",
    "pickle.dump(pac, open(filename,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
